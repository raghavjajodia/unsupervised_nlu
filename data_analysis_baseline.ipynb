{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank, ptb\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/rj1408/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/rj1408/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package ptb to /home/rj1408/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/ptb.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('ptb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49208, 1253013)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total number of words/sents in corpus\n",
    "len(ptb.tagged_sents()), len(ptb.tagged_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "allfileids = ptb.fileids()\n",
    "allfileids = list(map(lambda fid: (fid, fid.split('/')[1]), allfileids))\n",
    "trainfileids = list(filter(lambda tup: int(tup[1]) <= 18, allfileids))\n",
    "valfileids = list(filter(lambda tup: int(tup[1]) > 18 and int(tup[1]) <= 21, allfileids))\n",
    "testfileids = list(filter(lambda tup: int(tup[1]) >= 22, allfileids))\n",
    "trainfileids = list(map(lambda tup: tup[0], trainfileids))\n",
    "valfileids = list(map(lambda tup: tup[0], valfileids))\n",
    "testfileids = list(map(lambda tup: tup[0], testfileids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindict = {}\n",
    "traindict['tagged_words'] = list(ptb.tagged_words(fileids=trainfileids))\n",
    "traindict['tagged_sents'] = list(ptb.tagged_sents(fileids=trainfileids))\n",
    "valdict = {}\n",
    "valdict['tagged_words'] = list(ptb.tagged_words(fileids=valfileids))\n",
    "valdict['tagged_sents'] = list(ptb.tagged_sents(fileids=valfileids))\n",
    "testdict = {}\n",
    "testdict['tagged_words'] = list(ptb.tagged_words(fileids=testfileids))\n",
    "testdict['tagged_sents'] = list(ptb.tagged_sents(fileids=testfileids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"/scratch/rj1408/pos_lm/ptb_wsj_pos/train.p\",\"wb\") as f:\n",
    "    pkl.dump( traindict, f)\n",
    "with open(\"/scratch/rj1408/pos_lm/ptb_wsj_pos/val.p\",\"wb\") as f:\n",
    "    pkl.dump( valdict, f)\n",
    "with open(\"/scratch/rj1408/pos_lm/ptb_wsj_pos/test.p\",\"wb\") as f:\n",
    "    pkl.dump( testdict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/scratch/rj1408/pos_lm/ptb_wsj_pos/train.p\",\"rb\") as f:\n",
    "    traindict = pkl.load(f)\n",
    "with open(\"/scratch/rj1408/pos_lm/ptb_wsj_pos/val.p\",\"rb\") as f:\n",
    "    valdict = pkl.load(f)\n",
    "with open(\"/scratch/rj1408/pos_lm/ptb_wsj_pos/test.p\",\"rb\") as f:\n",
    "    testdict = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag analysis\n",
    "with open('tagset.txt') as f:\n",
    "    alltags = f.read()\n",
    "\n",
    "alltags = list(map(lambda strline: strline.split('\\t')[1], alltags.split('\\n')))\n",
    "alltags = set(alltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 127563),\n",
       " ('IN', 94760),\n",
       " ('NNP', 87693),\n",
       " ('DT', 78777),\n",
       " ('JJ', 58957),\n",
       " ('NNS', 57860),\n",
       " ('CD', 34891),\n",
       " ('RB', 29621),\n",
       " ('VBD', 28311),\n",
       " ('VB', 25489),\n",
       " ('CC', 22832),\n",
       " ('TO', 21462),\n",
       " ('VBZ', 20982),\n",
       " ('VBN', 19333),\n",
       " ('PRP', 16766),\n",
       " ('VBG', 14350),\n",
       " ('VBP', 12326),\n",
       " ('MD', 9437),\n",
       " ('POS', 8284),\n",
       " ('PRP$', 7989),\n",
       " ('WDT', 4194),\n",
       " ('JJR', 3174),\n",
       " ('RP', 2515),\n",
       " ('NNPS', 2505),\n",
       " ('WP', 2285),\n",
       " ('WRB', 2051),\n",
       " ('JJS', 1867),\n",
       " ('RBR', 1675),\n",
       " ('EX', 833),\n",
       " ('RBS', 435),\n",
       " ('PDT', 333),\n",
       " ('FW', 224),\n",
       " ('WP$', 166),\n",
       " ('UH', 87),\n",
       " ('SYM', 55),\n",
       " ('LS', 47)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get distribution of tags\n",
    "tagcntr = Counter(map(lambda tupl: tupl[1], filter(lambda tup: tup[1] in alltags, traindict['tagged_words'])))\n",
    "tagcntr.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model - Majority class voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokenAccuracy(labels, preds):\n",
    "    alllabels = [tok for lis in labels for tok in lis]\n",
    "    allpreds= [tok for lis in preds for tok in lis]\n",
    "    cnt = 0\n",
    "    for i,tok in enumerate(alllabels):\n",
    "        if alllabels[i]==allpreds[i]:\n",
    "            cnt += 1\n",
    "    return float(cnt)/len(alllabels)    \n",
    "\n",
    "def getSentAccuracy(labels, preds):\n",
    "    cnt = 0\n",
    "    for i,sent in enumerate(labels):\n",
    "        if labels[i]==preds[i]:\n",
    "            cnt += 1\n",
    "    return float(cnt)/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruneNonLabels(labels, preds, alltags):\n",
    "    useless = list(map(lambda lis: list(filter(lambda tup: tup[1] not in alltags, list(enumerate(lis)))), labels))\n",
    "    useless = list(map(lambda lis: set(map(lambda tup: tup[0], lis)), useless))\n",
    "    prunedLabels = list(map(lambda lis: list(filter(lambda lab: lab in alltags, lis)), labels))\n",
    "    prunedPreds = list(map(lambda tup: list(filter(lambda tupl: tupl[0] not in useless[tup[0]], list(enumerate(tup[1])))), list(enumerate(preds))))\n",
    "    prunedPreds = list(map(lambda lis: list(map(lambda tupl: tupl[1], lis)), prunedPreds))\n",
    "    itemsLabels = [tok for lis in prunedLabels for tok in lis]\n",
    "    itemsPruned = [tok for lis in prunedPreds for tok in lis]\n",
    "    assert len(itemsLabels) == len(itemsPruned)\n",
    "    return prunedLabels, prunedPreds\n",
    "    \n",
    "def getPredictions(lisoflisof_tokens, wrd_cntr):\n",
    "    return list(map(lambda lisoftokens: list(map(lambda tok: \"-1\" if tok not in wrd_cntr else wrd_cntr[tok].most_common(1)[0][0], lisoftokens)), lisoflisof_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = traindict['tagged_words']\n",
    "wrd_cntr = {}\n",
    "for tup in lis:\n",
    "    if tup[0] not in wrd_cntr:\n",
    "        wrd_cntr[tup[0]] = Counter()\n",
    "    wrd_cntr[tup[0]][tup[1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = list(map(lambda lis: list(map(lambda tup: tup[0], lis)), testdict['tagged_sents']))\n",
    "predictions = getPredictions(test_sents, wrd_cntr)\n",
    "labels = list(map(lambda lis: list(map(lambda tup: tup[1], lis)), testdict['tagged_sents']))\n",
    "labels, predictions = pruneNonLabels(labels, predictions, alltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.90011926321834, 0.169718051995606)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTokenAccuracy(labels, predictions), getSentAccuracy(labels, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
